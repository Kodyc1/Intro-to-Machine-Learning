{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS178 WINTER 2017 Project\n",
    "# KODY CHEUNG 85737824\n",
    "# AARON CHING 28162665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "# import X training points with 15 features and Y training points\n",
    "X_data = np.genfromtxt(r\"C:\\Users\\kodyc\\OneDrive\\Documents\\Old UCI Classes\\CS 178\\Project\\X_train.txt\",delimiter=None)\n",
    "Y_data = np.genfromtxt(r\"C:\\Users\\kodyc\\OneDrive\\Documents\\Old UCI Classes\\CS 178\\Project\\Y_train.txt\",delimiter=None)\n",
    "\n",
    "# import X testing points\n",
    "X_test = np.genfromtxt(r\"C:\\Users\\kodyc\\OneDrive\\Documents\\Old UCI Classes\\CS 178\\Project\\X_test.txt\",delimiter=None)\n",
    "Test = X_test.shape[0]\n",
    "\n",
    "full_ensemble = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68673\n",
      "[[ 0.46        0.54      ]\n",
      " [ 0.78        0.22      ]\n",
      " [ 0.62        0.38      ]\n",
      " ..., \n",
      " [ 0.55333333  0.44666667]\n",
      " [ 0.54666667  0.45333333]\n",
      " [ 1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neighbors\n",
    "import sklearn.decomposition\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "\n",
    "# create K-nearest neighbor learner\n",
    "\n",
    "# Different K values for nearest points\n",
    "nearest = [1, 3, 5, 15, 55, 105]\n",
    "\n",
    "\n",
    "\n",
    "# Subsampling a smaller part of the data\n",
    "X_train, X_valid, Y_train, Y_valid = ml.splitData(X_data, Y_data, 0.80)\n",
    "\n",
    "parameters = {'n_neighbors': nearest}\n",
    "\n",
    "knearest = sklearn.neighbors.KNeighborsClassifier()\n",
    "\n",
    "clf = sklearn.model_selection.GridSearchCV(knearest, parameters, cv = 10)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "dimensions = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "accuracy = []\n",
    "params = []\n",
    "\n",
    "\n",
    "\n",
    "Test = X_test.shape[0]\n",
    "\n",
    "predict = np.zeros((Test,2))\n",
    "\n",
    "\n",
    "for d in dimensions:\n",
    "    svd = sklearn.decomposition.TruncatedSVD(n_components = d)\n",
    "\n",
    "    X_fit = svd.fit_transform(X_train)\n",
    "    X_fit_atest = svd.transform(X_valid)\n",
    "\n",
    "    clf.fit(X_fit, Y_train)\n",
    "\n",
    "    Kfolds = sklearn.cross_validation.KFold(X_fit_atest.shape[0], n_folds = 10)\n",
    "    scores = []\n",
    "\n",
    "    for i,j in Kfolds:\n",
    "        test_set = X_fit_atest[j]\n",
    "        test_labels = Y_valid[j]\n",
    "        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_set)))\n",
    "        \n",
    "    accuracy.append(scores)\n",
    "    params.append(clf.best_params_['n_neighbors'])\n",
    "\n",
    "    test = svd.transform(X_test)\n",
    "    predict += clf.predict_proba(test)\n",
    "\n",
    "print(np.mean(accuracy))\n",
    "predict = predict/10\n",
    "print(predict)\n",
    "\n",
    "knn = predict\n",
    "\n",
    "# np.savetxt('1A-KNN.txt', np.vstack( (np.arange(len(predict)), predict[:,1]) ).T, \n",
    "#            '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SVM Kernel - libSVM but for large amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  100\n",
      "gamma:  1e-05\n",
      "Training Accuracy 0.72065\n",
      "Validation Accuracy 0.6995\n",
      "[[ 0.69178603  0.30821397]\n",
      " [ 0.70296301  0.29703699]\n",
      " [ 0.44241791  0.55758209]\n",
      " ..., \n",
      " [ 0.62448439  0.37551561]\n",
      " [ 0.6972542   0.3027458 ]\n",
      " [ 0.71792562  0.28207438]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = ml.splitData(X_data, Y_data, 0.80)\n",
    "\n",
    "#print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "cc = 100\n",
    "gamm = 0.00001\n",
    "svm_learner = svm.SVC(C = cc, gamma = gamm, probability = True)\n",
    "print(\"C: \", cc)\n",
    "print(\"gamma: \", gamm)\n",
    "svm_learner.fit(X_train, Y_train)\n",
    "\n",
    "Yhat = svm_learner.predict(X_valid)\n",
    "Ythat = svm_learner.predict(X_train)\n",
    "\n",
    "predictions = svm_learner.predict_proba(X_test)\n",
    "\n",
    "print('Training Accuracy', np.mean (Ythat == Y_train))\n",
    "print('Validation Accuracy', np.mean(Yhat == Y_valid))\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "np.savetxt('SVM_predict.txt', np.vstack( (np.arange(len(predictions)), predictions[:,1]) ).T, \n",
    "            '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n",
    "\n",
    "svmz = predictions\n",
    "\n",
    "#C value determines how willing you are to misclassify data when\n",
    "#deciding on the margin (higher the C, the less incorrect points are allowed within the margin)\n",
    "\n",
    "#its said that a linear function aka no kernel can help prevent overfitting\n",
    "# where a non linear function normally would, but that never finished running\n",
    "\n",
    "#small gamma gives low bias and high variance while large gamma will give\n",
    "#higher bias and low variance\n",
    "\n",
    "\n",
    "\n",
    "# mu = Y_train.mean()\n",
    "# dY = Yt - mu\n",
    "# step = 0.5\n",
    "\n",
    "# Pt = np.zeros((X_train.shape[0],)) + mu\n",
    "# Pv = np.zeros((X_valid.shape[0],)) + mu\n",
    "# Pe = np.zeros((X_test.shape[0],)) + mu\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# for i in range(0,25):\n",
    "    \n",
    "#     svm_learner = svm.SVC(probability = True)\n",
    "    \n",
    "#     svm_learner.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "#     Pt += step * svm_learner.predict(X_train)[:,0]\n",
    "#     Pv += step * svm_learner.predict(X_valid)[:,0]\n",
    "#     Pe += step * svm_learner.predict(X_test)[:,0]\n",
    "    \n",
    "#     dY -= step * svm_learner.predict(X_train)[:,0]\n",
    "    \n",
    "#     print(auc(X_valid, Y_valid))\n",
    "    \n",
    "    \n",
    "\n",
    "# Yhat = svm_learner.predict(X_valid)\n",
    "\n",
    "# predictions = svm_learner.predict_proba(X_valid)\n",
    "\n",
    "# print('Accuracy', np.mean(Yhat == Y_valid))\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# # df = pd.DataFrame(X_train)\n",
    "# # df['rain'] = Y_train\n",
    "# # sns.pairplot(df, hue='rain', vars = range(14))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #print(\"SVM Kernel AUC: {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Random forest of treeClassifiers\n",
    "### MaxDepth = 20, MinLeaf = 4, nFeatures = 10\n",
    "### 50 bag ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.7703160000000001\n",
      "validation error: 0.7709440000000001\n",
      "Random Forest AUC: 0.8136594140704936\n",
      "[[ 0.54922542  0.45077458]\n",
      " [ 0.69265074  0.30734926]\n",
      " [ 0.5900687   0.4099313 ]\n",
      " ..., \n",
      " [ 0.37693628  0.62306372]\n",
      " [ 0.43389854  0.56610146]\n",
      " [ 0.78407633  0.21592367]]\n"
     ]
    }
   ],
   "source": [
    "# dimensions of X_test\n",
    "m,n = X_test.shape\n",
    "\n",
    "Test = X_test.shape[0]\n",
    "\n",
    "# ensemble of classifiers\n",
    "bags = 50\n",
    "full_ensemble = [None] * bags\n",
    "Area_Under_Curve = []\n",
    "\n",
    "X_train = X_data[0:10000]\n",
    "X_valid = X_data[10000:20000]\n",
    "\n",
    "Y_train = Y_data[0:10000]\n",
    "Y_valid = Y_data[10000:20000]\n",
    "\n",
    "\n",
    "for i in range(0, bags):\n",
    "    \n",
    "    indices = np.floor(m * np.random.rand(m)).astype(int)    # random combination of 100k rows\n",
    "    \n",
    "    Xi, Yi = X_data[indices,:], Y_data[indices]              # X and Y indices of those rows\n",
    "    \n",
    "    # put the learners in the ensemble\n",
    "    full_ensemble[i] = ml.dtree.treeClassify(Xi, Yi, maxDepth = 20, minLeaf = 4, nFeatures = 10) \n",
    "    \n",
    "\n",
    "# space for predictions from each model\n",
    "predict = np.zeros((Test,2))\n",
    "\n",
    "train = []\n",
    "valid = []\n",
    "\n",
    "\n",
    "for i in range(0,bags):\n",
    "    \n",
    "    train.append(np.mean(full_ensemble[i].predict(X_train) == Y_train))\n",
    "    \n",
    "    valid.append(np.mean(full_ensemble[i].predict(X_valid) == Y_valid))    \n",
    "    \n",
    "    # soft predict on each learner in the bag\n",
    "    predict += full_ensemble[i].predictSoft(X_test)\n",
    "    \n",
    "    # get the auc of each learner in the bag\n",
    "    Area_Under_Curve.append(full_ensemble[i].auc(X_data, Y_data))\n",
    "\n",
    "# average predictions    \n",
    "predict = predict/50\n",
    "\n",
    "print(\"training error: {}\".format(np.mean(train)))\n",
    "print(\"validation error: {}\".format(np.mean(valid)))\n",
    "\n",
    "# average auc\n",
    "result = np.mean(Area_Under_Curve)\n",
    "print(\"Random Forest AUC: {}\".format(result))\n",
    "\n",
    "\n",
    "random_forest_bags = predict\n",
    "\n",
    "print(random_forest_bags)\n",
    "\n",
    "\n",
    "# np.savetxt('Yhat_ensemble.txt', np.vstack( (np.arange(len(predict)), predict[:,1]) ).T, \n",
    "#            '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Boosted Learner (Adaptive boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00445342  0.99554658]\n",
      " [ 0.93364847  0.06635153]\n",
      " [ 0.27200314  0.72799686]\n",
      " ..., \n",
      " [ 0.47389618  0.52610382]\n",
      " [ 0.83983735  0.16016265]\n",
      " [ 0.92960291  0.07039709]]\n",
      "0.667905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#GaussianNaiveBayes = GaussianNB()  # low rate of success\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "AUC = []\n",
    "\n",
    "Test = X_test.shape[0]\n",
    "ultraboost = np.zeros((Test,2))\n",
    "\n",
    "ensemble = [None] * 25\n",
    "\n",
    "for i in range(0, 25):\n",
    "    \n",
    "    Xtr, Xva, Ytr, Yva = train_test_split(X_data, Y_data, test_size=0.80, random_state = 42)\n",
    "\n",
    "    \n",
    "    ensemble[i] = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 15))\n",
    "    \n",
    "    ensemble[i].fit(Xtr, Ytr)\n",
    "\n",
    "\n",
    "    AUC.append(np.mean(ensemble[i].predict(Xva) == Yva))\n",
    "    \n",
    "    ultraboost += ensemble[i].predict_proba(X_test)\n",
    "    \n",
    "    #print(\"training error: {}\".format(np.mean(ensemble.predict(Xtr) == Ytr)))\n",
    "\n",
    "ultraboost = ultraboost/25\n",
    "print(ultraboost)\n",
    "print(np.mean(AUC))\n",
    "\n",
    "\n",
    "# np.savetxt('Ultraboost.txt', np.vstack( (np.arange(len(ultraboost)), ultraboost[:,1]) ).T, \n",
    "#            '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n",
    "    \n",
    "#print(\"AdaBoost AUC: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.12812779e-01   8.71872213e-02]\n",
      " [  9.90319901e-01   9.68009874e-03]\n",
      " [  9.45868235e-01   5.41317650e-02]\n",
      " ..., \n",
      " [  7.18525356e-04   9.99281475e-01]\n",
      " [  2.50928983e-03   9.97490710e-01]\n",
      " [  9.32473504e-01   6.75264962e-02]]\n",
      "Training Accuracy 0.9662875\n",
      "Validation Accuracy 0.69995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = ml.splitData(X_data, Y_data, 0.80)\n",
    "\n",
    "Test = X_test.shape[0]\n",
    "predict = np.zeros((Test, 2))\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=3000, learning_rate=1, max_depth=500\n",
    "                              ,max_leaf_nodes=50)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Yhat = (clf.predict(X_valid))\n",
    "Ythat = (clf.predict(X_train))\n",
    "gradiboost = clf.predict_proba(X_test)\n",
    "\n",
    "print(gradiboost)\n",
    "print('Training Accuracy', np.mean(Ythat == Y_train))\n",
    "print('Validation Accuracy', np.mean(Yhat == Y_valid))\n",
    "np.savetxt('GB_predict.txt', np.vstack( (np.arange(len(gradiboost)), gradiboost[:,1]) ).T, \n",
    "            '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n",
    "\n",
    "#combines a set of weak learners using a gradient descent-like procedure, outcomes are weighed based on the previous instant\n",
    "#misclassifed outcomes will have higher weight\n",
    "\n",
    "#Regression trees are used as weak learners, and their outputs are added together and correct the residuals in the predictions\n",
    "#(based on a loss function of your choice). A gradient descent procedure is used to minimize the loss when\n",
    "#adding the trees together.\n",
    "#We put constraints on the trees to make sure they stay weak. We also weight the predictions of each tree to slow down the\n",
    "#learning of the algorithm.\n",
    "\n",
    "#learning rate: impact of each tree on the outcome. Magnitude of changes based\n",
    "#on the outputs of the trees\n",
    "#Lower values make the model robust to tree characteristics, and requires more\n",
    "#trees to model all relations (makes it more expensive)\n",
    "\n",
    "#n_estimators: number of sequential trees to be modeled\n",
    "#GBM is pretty robust against overfitting, but still will at some point (should\n",
    "#tune n_estimators for a particular learning rate)\n",
    "\n",
    "#max_depth: limits the number of nodes in the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.6477533333333334\n",
      "0.64864\n",
      "[[ 0.91895889  0.08104111]\n",
      " [ 0.85292407  0.14707593]\n",
      " [ 0.81029217  0.18970783]\n",
      " ..., \n",
      " [ 0.41457393  0.58542607]\n",
      " [ 0.91473217  0.08526783]\n",
      " [ 0.8715349   0.1284651 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\"\"\" solver = 'adam' for large data sets \"\"\"\n",
    "\n",
    "Test = X_test.shape[0]\n",
    "neural_net = np.zeros((Test,2))\n",
    "\n",
    "train = []\n",
    "average = []\n",
    "\n",
    "nFolds = 10;\n",
    "\n",
    "for iFold in range(nFolds):\n",
    "\n",
    "    Xtr, Xva, Ytr, Yva = ml.crossValidate(X_data, Y_data, nFolds, iFold)\n",
    "\n",
    "    neural_network = MLPClassifier(solver = 'adam', random_state = 0)\n",
    "\n",
    "    neural_network.fit(Xtr, Ytr)\n",
    "\n",
    "    predict += neural_network.predict_proba(X_test)\n",
    "    \n",
    "    train.append(np.mean(neural_network.predict(Xtr) == Ytr))\n",
    "\n",
    "    average.append(np.mean(neural_network.predict(Xva) == Yva))\n",
    "\n",
    "    #print(Yhat)\n",
    "\n",
    "print(\"training error: {}\".format(np.mean(train)))\n",
    "    \n",
    "print(np.mean(average))\n",
    "predict = predict/10\n",
    "neural_net = predict\n",
    "print(neural_net)\n",
    "\n",
    "##np.savetxt('NNET.txt', np.vstack( (np.arange(len(predict)), predict[:,1]) ).T,\n",
    "##           '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n",
    "\n",
    "#print(\"Neural Network AUC: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination bag of all learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46        0.54      ]\n",
      " [ 0.78        0.22      ]\n",
      " [ 0.62        0.38      ]\n",
      " ..., \n",
      " [ 0.55333333  0.44666667]\n",
      " [ 0.54666667  0.45333333]\n",
      " [ 1.          0.        ]]\n",
      "\n",
      "[[ 0.69178603  0.30821397]\n",
      " [ 0.70296301  0.29703699]\n",
      " [ 0.44241791  0.55758209]\n",
      " ..., \n",
      " [ 0.62448439  0.37551561]\n",
      " [ 0.6972542   0.3027458 ]\n",
      " [ 0.71792562  0.28207438]]\n",
      "\n",
      "[[ 0.54922542  0.45077458]\n",
      " [ 0.69265074  0.30734926]\n",
      " [ 0.5900687   0.4099313 ]\n",
      " ..., \n",
      " [ 0.37693628  0.62306372]\n",
      " [ 0.43389854  0.56610146]\n",
      " [ 0.78407633  0.21592367]]\n",
      "\n",
      "[[ 0.00445342  0.99554658]\n",
      " [ 0.93364847  0.06635153]\n",
      " [ 0.27200314  0.72799686]\n",
      " ..., \n",
      " [ 0.47389618  0.52610382]\n",
      " [ 0.83983735  0.16016265]\n",
      " [ 0.92960291  0.07039709]]\n",
      "\n",
      "[[  9.12812779e-01   8.71872213e-02]\n",
      " [  9.90319901e-01   9.68009874e-03]\n",
      " [  9.45868235e-01   5.41317650e-02]\n",
      " ..., \n",
      " [  7.18525356e-04   9.99281475e-01]\n",
      " [  2.50928983e-03   9.97490710e-01]\n",
      " [  9.32473504e-01   6.75264962e-02]]\n",
      "\n",
      "[[ 0.91895889  0.08104111]\n",
      " [ 0.85292407  0.14707593]\n",
      " [ 0.81029217  0.18970783]\n",
      " ..., \n",
      " [ 0.41457393  0.58542607]\n",
      " [ 0.91473217  0.08526783]\n",
      " [ 0.8715349   0.1284651 ]]\n",
      "\n",
      "[[ 0.58953942  0.41046058]\n",
      " [ 0.8254177   0.1745823 ]\n",
      " [ 0.61344169  0.38655831]\n",
      " ..., \n",
      " [ 0.40732377  0.59267623]\n",
      " [ 0.57248304  0.42751696]\n",
      " [ 0.87260221  0.12739779]]\n"
     ]
    }
   ],
   "source": [
    "# full_ensemble + random_forest_bags + ultraboost + neural_net\n",
    "\n",
    "# knn\n",
    "print(knn)\n",
    "print()\n",
    "\n",
    "#svm\n",
    "print(svmz)\n",
    "print()\n",
    "\n",
    "# random_forest_bags\n",
    "print(random_forest_bags)\n",
    "print()\n",
    "\n",
    "# ultraboost\n",
    "print(ultraboost)\n",
    "print()\n",
    "\n",
    "# gradiboost\n",
    "print(gradiboost)\n",
    "print()\n",
    "\n",
    "\n",
    "# neural_net\n",
    "print(neural_net)\n",
    "print()\n",
    "\n",
    "\n",
    "full_ensemble = (knn + random_forest_bags + ultraboost + neural_net + svmz + gradiboost)/6\n",
    "\n",
    "print(full_ensemble)\n",
    "\n",
    "\n",
    "np.savetxt('6combination.txt', np.vstack( (np.arange(len(full_ensemble)), full_ensemble[:,1]) ).T,\n",
    "          '%d, %.2f', header = 'ID,Prob1', comments = '', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "# Summation of soft predicts / number of soft predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
